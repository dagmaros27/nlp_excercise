{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dagmaros27/AIMS_Notebooks/blob/main/CUDA_Practical_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CUDA Programming on NVIDIA GPUs**\n",
        "\n",
        "# **Practical 2**\n",
        "\n",
        "Again make sure the correct Runtime is being used, by clicking on the Runtime option at the top, then \"Change runtime type\", and selecting an appropriate GPU such as the T4.\n",
        "\n",
        "Then verify with the instruction below the details of the GPU which is available to you.  "
      ],
      "metadata": {
        "id": "i1JlUA_e44zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uboEpcMD4xYA",
        "outputId": "25d494f1-4f02-490e-d859-de01a0a08eb4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan 15 12:52:50 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "First we upload two header files from the course webpage."
      ],
      "metadata": {
        "id": "nlO6dHwW7gRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_cuda.h\n",
        "!wget https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_string.h\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv1nyjTmTmr7",
        "outputId": "d6a85c0a-0c36-4a75-86b6-255e821c748e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-15 12:52:53--  https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_cuda.h\n",
            "Resolving people.maths.ox.ac.uk (people.maths.ox.ac.uk)... 129.67.184.129, 2001:630:441:201::8143:b881\n",
            "Connecting to people.maths.ox.ac.uk (people.maths.ox.ac.uk)|129.67.184.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27832 (27K) [text/x-chdr]\n",
            "Saving to: ‘helper_cuda.h’\n",
            "\n",
            "helper_cuda.h       100%[===================>]  27.18K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2026-01-15 12:52:54 (198 KB/s) - ‘helper_cuda.h’ saved [27832/27832]\n",
            "\n",
            "--2026-01-15 12:52:54--  https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_string.h\n",
            "Resolving people.maths.ox.ac.uk (people.maths.ox.ac.uk)... 129.67.184.129, 2001:630:441:201::8143:b881\n",
            "Connecting to people.maths.ox.ac.uk (people.maths.ox.ac.uk)|129.67.184.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14875 (15K) [text/x-chdr]\n",
            "Saving to: ‘helper_string.h’\n",
            "\n",
            "helper_string.h     100%[===================>]  14.53K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2026-01-15 12:52:54 (368 KB/s) - ‘helper_string.h’ saved [14875/14875]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "The next step is to create the file prac2.cu."
      ],
      "metadata": {
        "id": "RD6IjBwY2Ltm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile prac2.cu\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU version of Monte Carlo algorithm using NVIDIA's CURAND library\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <curand.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CUDA global constants\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__constant__ int   N;\n",
        "__constant__ float T, r, sigma, rho, alpha, dt, con1, con2;\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// kernel routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "\n",
        "__global__ void pathcalc(float *d_z, float *d_v)\n",
        "{\n",
        "  float s1, s2, y1, y2, payoff;\n",
        "  int   ind;\n",
        "\n",
        "  // move array pointers to correct position\n",
        "\n",
        "  // version 1\n",
        "  ind = threadIdx.x + 2*N*blockIdx.x*blockDim.x;\n",
        "\n",
        "  // version 2\n",
        "  // ind = 2*N*threadIdx.x + 2*N*blockIdx.x*blockDim.x;\n",
        "\n",
        "\n",
        "  // path calculation\n",
        "\n",
        "  s1 = 1.0f;\n",
        "  s2 = 1.0f;\n",
        "\n",
        "  for (int n=0; n<N; n++) {\n",
        "    y1   = d_z[ind];\n",
        "    // version 1\n",
        "    ind += blockDim.x;      // shift pointer to next element\n",
        "    // version 2\n",
        "    // ind += 1;\n",
        "\n",
        "    y2   = rho*y1 + alpha*d_z[ind];\n",
        "    // version 1\n",
        "    ind += blockDim.x;      // shift pointer to next element\n",
        "    // version 2\n",
        "    // ind += 1;\n",
        "\n",
        "    s1 = s1*(con1 + con2*y1);\n",
        "    s2 = s2*(con1 + con2*y2);\n",
        "  }\n",
        "\n",
        "  // put payoff value into device array\n",
        "\n",
        "  payoff = 0.0f;\n",
        "  if ( fabs(s1-1.0f)<0.1f && fabs(s2-1.0f)<0.1f ) payoff = exp(-r*T);\n",
        "\n",
        "  d_v[threadIdx.x + blockIdx.x*blockDim.x] = payoff;\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main(int argc, const char **argv){\n",
        "\n",
        "  int     NPATH=9600000, h_N=100;\n",
        "  float   h_T, h_r, h_sigma, h_rho, h_alpha, h_dt, h_con1, h_con2;\n",
        "  float  *h_v, *d_v, *d_z;\n",
        "  double  sum1, sum2;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  // initialise CUDA timing\n",
        "\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // allocate memory on host and device\n",
        "\n",
        "  h_v = (float *)malloc(sizeof(float)*NPATH);\n",
        "\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_v, sizeof(float)*NPATH) );\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_z, sizeof(float)*2*h_N*NPATH) );\n",
        "\n",
        "  // define constants and transfer to GPU\n",
        "\n",
        "  h_T     = 1.0f;\n",
        "  h_r     = 0.05f;\n",
        "  h_sigma = 0.1f;\n",
        "  h_rho   = 0.5f;\n",
        "  h_alpha = sqrt(1.0f-h_rho*h_rho);\n",
        "  h_dt    = 1.0f/h_N;\n",
        "  h_con1  = 1.0f + h_r*h_dt;\n",
        "  h_con2  = sqrt(h_dt)*h_sigma;\n",
        "\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(N,    &h_N,    sizeof(h_N)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(T,    &h_T,    sizeof(h_T)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(r,    &h_r,    sizeof(h_r)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(sigma,&h_sigma,sizeof(h_sigma)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(rho,  &h_rho,  sizeof(h_rho)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(alpha,&h_alpha,sizeof(h_alpha)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(dt,   &h_dt,   sizeof(h_dt)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(con1, &h_con1, sizeof(h_con1)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(con2, &h_con2, sizeof(h_con2)) );\n",
        "\n",
        "  // random number generation\n",
        "\n",
        "  curandGenerator_t gen;\n",
        "  checkCudaErrors( curandCreateGenerator(&gen, CURAND_RNG_PSEUDO_DEFAULT) );\n",
        "  checkCudaErrors( curandSetPseudoRandomGeneratorSeed(gen, 1234ULL) );\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( curandGenerateNormal(gen, d_z, 2*h_N*NPATH, 0.0f, 1.0f) );\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  printf(\"CURAND normal RNG  execution time (ms): %f,  samples/sec: %e \\n\",\n",
        "          milli, 2.0*h_N*NPATH/(0.001*milli));\n",
        "\n",
        "  // execute kernel and time it\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  pathcalc<<<NPATH/128, 128>>>(d_z, d_v);\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  getLastCudaError(\"pathcalc execution failed\\n\");\n",
        "  printf(\"Monte Carlo kernel execution time (ms): %f \\n\",milli);\n",
        "\n",
        "  // copy back results\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( cudaMemcpy(h_v, d_v, sizeof(float)*NPATH,\n",
        "                   cudaMemcpyDeviceToHost) );\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"Device to host data transfer time (ms): %f \\n\",milli);\n",
        "\n",
        "  float transferTime = milli/1000.0f; // in seconds\n",
        "  float dataSize = (sizeof(float)*NPATH) / (1024.0f*1024.0f*1024.0f); // in GB\n",
        "  printf(\"Data transfer speed (GB/s):  %f\", dataSize/transferTime);\n",
        "\n",
        "  // compute average\n",
        "\n",
        "  sum1 = 0.0;\n",
        "  sum2 = 0.0;\n",
        "  for (int i=0; i<NPATH; i++) {\n",
        "    sum1 += h_v[i];\n",
        "    sum2 += h_v[i]*h_v[i];\n",
        "  }\n",
        "\n",
        "  printf(\"\\nAverage value and standard deviation of error  = %13.8f %13.8f\\n\\n\",\n",
        "\t sum1/NPATH, sqrt((sum2/NPATH - (sum1/NPATH)*(sum1/NPATH))/NPATH) );\n",
        "\n",
        "  // Tidy up library\n",
        "\n",
        "  checkCudaErrors( curandDestroyGenerator(gen) );\n",
        "\n",
        "  // Release memory and exit cleanly\n",
        "\n",
        "  free(h_v);\n",
        "  checkCudaErrors( cudaFree(d_v) );\n",
        "  checkCudaErrors( cudaFree(d_z) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcwQANS22i3Q",
        "outputId": "db1fe84b-da2b-4a60-a01e-b96f196b86a1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting prac2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "We can now compile and run the executable.  Note that the compilation links in the CUDA random number generation library cuRAND.\n"
      ],
      "metadata": {
        "id": "yds03ug532rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc prac2.cu -o prac2 -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart -lcurand"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFHWm4Dd3_hw",
        "outputId": "9181630f-dd86-4f6d-ca38-5f78f0436a18"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem, 36 bytes cmem[3]\n",
            "ptxas info    : Compiling entry function '_Z8pathcalcPfS_' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z8pathcalcPfS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 30 registers, 368 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./prac2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7jX9dSAaLj0",
        "outputId": "48c50bb9-e2e0-44fb-9dd9-6bb1c7a0c98b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "CURAND normal RNG  execution time (ms): 105.433853,  samples/sec: 1.821047e+10 \n",
            "Monte Carlo kernel execution time (ms): 29.521919 \n",
            "Device to host data transfer time (ms): 25.711712 \n",
            "Data transfer speed (GB/s):  1.390914\n",
            "Average value and standard deviation of error  =    0.41786269    0.00015237\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2nd version (Bad version)"
      ],
      "metadata": {
        "id": "cBmGtoMIiTg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile prac2v2.cu\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU version of Monte Carlo algorithm using NVIDIA's CURAND library\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <curand.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CUDA global constants\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__constant__ int   N;\n",
        "__constant__ float T, r, sigma, rho, alpha, dt, con1, con2;\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// kernel routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "\n",
        "__global__ void pathcalc(float *d_z, float *d_v)\n",
        "{\n",
        "  float s1, s2, y1, y2, payoff;\n",
        "  int   ind;\n",
        "\n",
        "  // move array pointers to correct position\n",
        "\n",
        "  // version 1\n",
        "  // ind = threadIdx.x + 2*N*blockIdx.x*blockDim.x;\n",
        "\n",
        "  // version 2\n",
        "  ind = 2*N*threadIdx.x + 2*N*blockIdx.x*blockDim.x;\n",
        "\n",
        "\n",
        "  // path calculation\n",
        "\n",
        "  s1 = 1.0f;\n",
        "  s2 = 1.0f;\n",
        "\n",
        "  for (int n=0; n<N; n++) {\n",
        "    y1   = d_z[ind];\n",
        "    // version 1\n",
        "    // ind += blockDim.x;      // shift pointer to next element\n",
        "    // version 2\n",
        "    ind += 1;\n",
        "\n",
        "    y2   = rho*y1 + alpha*d_z[ind];\n",
        "    // version 1\n",
        "    // ind += blockDim.x;      // shift pointer to next element\n",
        "    // version 2\n",
        "    ind += 1;\n",
        "\n",
        "    s1 = s1*(con1 + con2*y1);\n",
        "    s2 = s2*(con1 + con2*y2);\n",
        "  }\n",
        "\n",
        "  // put payoff value into device array\n",
        "\n",
        "  payoff = 0.0f;\n",
        "  if ( fabs(s1-1.0f)<0.1f && fabs(s2-1.0f)<0.1f ) payoff = exp(-r*T);\n",
        "\n",
        "  d_v[threadIdx.x + blockIdx.x*blockDim.x] = payoff;\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main(int argc, const char **argv){\n",
        "\n",
        "  int     NPATH=9600000, h_N=100;\n",
        "  float   h_T, h_r, h_sigma, h_rho, h_alpha, h_dt, h_con1, h_con2;\n",
        "  float  *h_v, *d_v, *d_z;\n",
        "  double  sum1, sum2;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  // initialise CUDA timing\n",
        "\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // allocate memory on host and device\n",
        "\n",
        "  h_v = (float *)malloc(sizeof(float)*NPATH);\n",
        "\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_v, sizeof(float)*NPATH) );\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_z, sizeof(float)*2*h_N*NPATH) );\n",
        "\n",
        "  // define constants and transfer to GPU\n",
        "\n",
        "  h_T     = 1.0f;\n",
        "  h_r     = 0.05f;\n",
        "  h_sigma = 0.1f;\n",
        "  h_rho   = 0.5f;\n",
        "  h_alpha = sqrt(1.0f-h_rho*h_rho);\n",
        "  h_dt    = 1.0f/h_N;\n",
        "  h_con1  = 1.0f + h_r*h_dt;\n",
        "  h_con2  = sqrt(h_dt)*h_sigma;\n",
        "\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(N,    &h_N,    sizeof(h_N)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(T,    &h_T,    sizeof(h_T)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(r,    &h_r,    sizeof(h_r)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(sigma,&h_sigma,sizeof(h_sigma)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(rho,  &h_rho,  sizeof(h_rho)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(alpha,&h_alpha,sizeof(h_alpha)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(dt,   &h_dt,   sizeof(h_dt)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(con1, &h_con1, sizeof(h_con1)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(con2, &h_con2, sizeof(h_con2)) );\n",
        "\n",
        "  // random number generation\n",
        "\n",
        "  curandGenerator_t gen;\n",
        "  checkCudaErrors( curandCreateGenerator(&gen, CURAND_RNG_PSEUDO_DEFAULT) );\n",
        "  checkCudaErrors( curandSetPseudoRandomGeneratorSeed(gen, 1234ULL) );\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( curandGenerateNormal(gen, d_z, 2*h_N*NPATH, 0.0f, 1.0f) );\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  printf(\"CURAND normal RNG  execution time (ms): %f,  samples/sec: %e \\n\",\n",
        "          milli, 2.0*h_N*NPATH/(0.001*milli));\n",
        "\n",
        "  // execute kernel and time it\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  pathcalc<<<NPATH/128, 128>>>(d_z, d_v);\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  getLastCudaError(\"pathcalc execution failed\\n\");\n",
        "  printf(\"Monte Carlo kernel execution time (ms): %f \\n\",milli);\n",
        "\n",
        "  // copy back results\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(h_v, d_v, sizeof(float)*NPATH,\n",
        "                   cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // compute average\n",
        "\n",
        "  sum1 = 0.0;\n",
        "  sum2 = 0.0;\n",
        "  for (int i=0; i<NPATH; i++) {\n",
        "    sum1 += h_v[i];\n",
        "    sum2 += h_v[i]*h_v[i];\n",
        "  }\n",
        "\n",
        "  printf(\"\\nAverage value and standard deviation of error  = %13.8f %13.8f\\n\\n\",\n",
        "\t sum1/NPATH, sqrt((sum2/NPATH - (sum1/NPATH)*(sum1/NPATH))/NPATH) );\n",
        "\n",
        "  // Tidy up library\n",
        "\n",
        "  checkCudaErrors( curandDestroyGenerator(gen) );\n",
        "\n",
        "  // Release memory and exit cleanly\n",
        "\n",
        "  free(h_v);\n",
        "  checkCudaErrors( cudaFree(d_v) );\n",
        "  checkCudaErrors( cudaFree(d_z) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63d01c6f-5688-40f6-b57d-24d35d28cd46",
        "id": "Oycyr-TxgB2R"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing prac2v2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc prac2v2.cu -o prac2v2 -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart -lcurand"
      ],
      "metadata": {
        "id": "zXRm5JtUirEL",
        "outputId": "55828ffc-f72a-428a-cb67-88503a1d4d38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem, 36 bytes cmem[3]\n",
            "ptxas info    : Compiling entry function '_Z8pathcalcPfS_' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z8pathcalcPfS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 32 registers, 368 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./prac2v2"
      ],
      "metadata": {
        "id": "ejrWCTyii0GJ",
        "outputId": "f298ed20-7812-43ee-8f21-60ec1542035e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "CURAND normal RNG  execution time (ms): 105.385315,  samples/sec: 1.821886e+10 \n",
            "Monte Carlo kernel execution time (ms): 100.261215 \n",
            "\n",
            "Average value and standard deviation of error  =    0.41793859    0.00015237\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Why in Version 1 the 32\n",
        "threads in a warp read in a consecutive block of 32 random numbers at the\n",
        "same time, but they don’t in Version 2?\n",
        "\n",
        "**Answer:** It is because we are changed the indexing in a way that doesn't correctly align the data with warp,so now threads in a singles warp might need different cache line to access data and this will decrease spatial locality"
      ],
      "metadata": {
        "id": "xxZz9CLgvulE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "By going back to the previous code block you can modify the code to complete the Practical 2 exercises as prescribed in the Instructions for this practical.\n",
        "\n",
        "It is perhaps better to copy the Code cell (on my system this is done by using the mouse right-click) and paste it to form a new Code cell, so that the original cell uses Version 1, and the new cell uses Version 2.  You can then make a further copy when you do the new application for the final step in the practical.\n",
        "\n",
        "Remember to first make your own copy of the notebook so that you are able to edit it.\n",
        "\n",
        "For students doing this as an assignment to be assessed, you should add your name to the title of the notebook (as in \"Practical 2 -- Mike Giles.ipynb\"), make it shared (see the Share option in the top-right corner) and provide the shared link as the submission mechanism.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Lastly, the instructions below create, compile and execute a version of the code which generates the random numbers on-the-fly within each kernel using the device-level cuRAND library.  This gives the very best performance because the random numbers no longer are being transferred between the GPU and the device memory.\n"
      ],
      "metadata": {
        "id": "ncymVLmd4L82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile prac2_device.cu\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU version of Monte Carlo algorithm using NVIDIA's CURAND library\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <curand_kernel.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CUDA global constants\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__constant__ int   N;\n",
        "__constant__ float T, r, sigma, rho, alpha, dt, con1, con2;\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// kernel routines -- see sections 3.5, 3.6 in cuRAND documentation\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void RNG_init(curandState *state)\n",
        "{\n",
        "  // RNG initialisation with id-based skipahead\n",
        "  int id = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "  curand_init(1234, id, 0, &state[id]);\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void pathcalc(curandState *device_state, float *d_v,\n",
        "                         int mpath, int NPATH)\n",
        "{\n",
        "  float s1, s2, y1, y2, payoff;\n",
        "\n",
        "  int id = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "  curandState_t state = device_state[id];\n",
        "\n",
        "  for(int m=0; m<mpath; m++) {\n",
        "    s1 = 1.0f;\n",
        "    s2 = 1.0f;\n",
        "\n",
        "    for (int n=0; n<N; n++) {\n",
        "      y1 = curand_normal(&state);\n",
        "      y2 = rho*y1 + alpha*curand_normal(&state);\n",
        "\n",
        "      s1 = s1*(con1 + con2*y1);\n",
        "      s2 = s2*(con1 + con2*y2);\n",
        "    }\n",
        "\n",
        "    // put payoff value into device array\n",
        "\n",
        "    payoff = 0.0f;\n",
        "    if ( fabs(s1-1.0f)<0.1f && fabs(s2-1.0f)<0.1f ) payoff = exp(-r*T);\n",
        "\n",
        "    int payoff_id = id + m*gridDim.x*blockDim.x;\n",
        "    if (payoff_id < NPATH) d_v[payoff_id] = payoff;\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main(int argc, const char **argv){\n",
        "\n",
        "  int     NPATH=9600000, h_N=100;\n",
        "  float   h_T, h_r, h_sigma, h_rho, h_alpha, h_dt, h_con1, h_con2;\n",
        "  float  *h_v, *d_v;\n",
        "  double  sum1, sum2;\n",
        "  curandState *state;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  // initialise CUDA timing\n",
        "\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // allocate memory on host and device\n",
        "\n",
        "  h_v = (float *)malloc(sizeof(float)*NPATH);\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_v, sizeof(float)*NPATH) );\n",
        "  checkCudaErrors( cudaMalloc((void **)&state, sizeof(curandState)*NPATH) );\n",
        "\n",
        "  // printf(\"size of curandState is %d bytes\\n\",sizeof(curandState));\n",
        "\n",
        "  // define constants and transfer to GPU\n",
        "\n",
        "  h_T     = 1.0f;\n",
        "  h_r     = 0.05f;\n",
        "  h_sigma = 0.1f;\n",
        "  h_rho   = 0.5f;\n",
        "  h_alpha = sqrt(1.0f-h_rho*h_rho);\n",
        "  h_dt    = 1.0f/h_N;\n",
        "  h_con1  = 1.0f + h_r*h_dt;\n",
        "  h_con2  = sqrt(h_dt)*h_sigma;\n",
        "\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(N,    &h_N,    sizeof(h_N)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(T,    &h_T,    sizeof(h_T)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(r,    &h_r,    sizeof(h_r)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(sigma,&h_sigma,sizeof(h_sigma)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(rho,  &h_rho,  sizeof(h_rho)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(alpha,&h_alpha,sizeof(h_alpha)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(dt,   &h_dt,   sizeof(h_dt)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(con1, &h_con1, sizeof(h_con1)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(con2, &h_con2, sizeof(h_con2)) );\n",
        "\n",
        "  // calculate theoretical occupancy -- see Pro Tip blog article:\n",
        "  // https://developer.nvidia.com/blog/cuda-pro-tip-occupancy-api-simplifies-launch-configuration/\n",
        "\n",
        "  int device;\n",
        "  cudaDeviceProp props;\n",
        "  cudaGetDevice(&device);\n",
        "  cudaGetDeviceProperties(&props, device);\n",
        "\n",
        "  int maxActiveBlocks, blockSize=128;\n",
        "  cudaOccupancyMaxActiveBlocksPerMultiprocessor( &maxActiveBlocks,\n",
        "                                                 pathcalc, blockSize, 0);\n",
        "  printf(\"maxActiveBlocks/SM = %d \\n\",maxActiveBlocks);\n",
        "  printf(\"number of SMs      = %d \\n\",props.multiProcessorCount);\n",
        "  int blocks = maxActiveBlocks*props.multiProcessorCount;\n",
        "\n",
        "  // execute kernels\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  RNG_init<<<blocks, 128>>>(state);\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  getLastCudaError(\"RNG_init execution failed\\n\");\n",
        "  printf(\"RNG_init kernel execution time (ms): %f \\n\",milli);\n",
        "\n",
        "  int paths_per_thread = (NPATH-1)/(128*blocks) + 1;\n",
        "  cudaEventRecord(start);\n",
        "  pathcalc<<<blocks, 128>>>(state,d_v,paths_per_thread,NPATH);\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  getLastCudaError(\"pathcalc execution failed\\n\");\n",
        "  printf(\"pathcalc kernel execution time (ms): %f \\n\",milli);\n",
        "\n",
        "  // copy back results\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(h_v, d_v, sizeof(float)*NPATH,\n",
        "                   cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // compute average\n",
        "\n",
        "  sum1 = 0.0;\n",
        "  sum2 = 0.0;\n",
        "  for (int i=0; i<NPATH; i++) {\n",
        "    sum1 += h_v[i];\n",
        "    sum2 += h_v[i]*h_v[i];\n",
        "  }\n",
        "\n",
        "  printf(\"\\nAverage value and standard deviation of error  = %13.8f %13.8f\\n\\n\",\n",
        "\t sum1/NPATH, sqrt((sum2/NPATH - (sum1/NPATH)*(sum1/NPATH))/NPATH) );\n",
        "\n",
        "  // Release memory and exit cleanly\n",
        "\n",
        "  free(h_v);\n",
        "  checkCudaErrors( cudaFree(d_v) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8xscLewbPlF",
        "outputId": "8b7c64a9-89b7-4627-b60f-90768548400a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing prac2_device.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc prac2_device.cu -o prac2_device -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart -lcurand"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzGXiohVT5OR",
        "outputId": "bd203d8f-abce-4c28-e119-d11da83ee01a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 218048 bytes gmem, 108 bytes cmem[3], 64 bytes cmem[4]\n",
            "ptxas info    : Compiling entry function '_Z8pathcalcP17curandStateXORWOWPfii' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z8pathcalcP17curandStateXORWOWPfii\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 29 registers, 376 bytes cmem[0]\n",
            "ptxas info    : Compiling entry function '_Z8RNG_initP17curandStateXORWOW' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z8RNG_initP17curandStateXORWOW\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 32 registers, 360 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./prac2_device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsfyCekjazPh",
        "outputId": "65881855-7ecb-4ea2-dedd-8e0802521727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "maxActiveBlocks/SM = 8 \n",
            "number of SMs      = 40 \n",
            "RNG_init kernel execution time (ms): 1.667104 \n",
            "pathcalc kernel execution time (ms): 23.665888 \n",
            "\n",
            "Average value and standard deviation of error  =    0.41802440    0.00015237\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again the final piece of code is to terminate the runtime."
      ],
      "metadata": {
        "id": "Fa5hiDuchY7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "Q8twCtvWhd9q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}